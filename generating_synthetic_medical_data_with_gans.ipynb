{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253e26bc",
   "metadata": {},
   "source": [
    "# Generating Synthetic Medical Data with GANs\n",
    "*A Deep Learning Approach for Privacy-Preserving Healthcare Analytics*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2129f9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook contains the end-to-end implementation for my dissertation, which explores the use of Generative Adversarial Networks (GANs) to generate synthetic medical datasets.  \n",
    "The aim is to create data that retains the statistical properties and predictive utility of real patient records, while ensuring privacy and reducing the risk of re-identification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62fbeb",
   "metadata": {},
   "source": [
    "# Installing all dependencies Required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce41557",
   "metadata": {},
   "source": [
    "## Data Preparation: Merging and UCI Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d287e",
   "metadata": {},
   "source": [
    "The original UCI Diabetes 130-US Hospitals dataset is split into two files:  \n",
    "1. **diabetic_data.csv** – Contains patient-level diabetic and hospitalization records.  \n",
    "2. **IDS_mapping.csv** – Contains mapping tables that convert coded identifiers (eg: admission_type_id) into descriptive categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b952653",
   "metadata": {},
   "source": [
    "In this section, we:\n",
    "- Merge both csv files.\n",
    "- Handle missing values and remove irrelevant columns.\n",
    "- Encode categorical features into numerical form for downstream GAN training.\n",
    "- Save the cleaned dataset for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42477353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned dataset saved to: Dataset/MergedDataset/diabetic_data_cleaned_for_Imputation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Loading the csv files\n",
    "diabetic_df = pd.read_csv('Dataset/rawData/diabetic_data.csv')\n",
    "ids_mapping_df = pd.read_csv('Dataset/rawData/IDS_mapping.csv')\n",
    "\n",
    "# Removing irrelevant or high missing value columns\n",
    "columns_to_drop = ['weight', 'payer_code', 'medical_specialty']\n",
    "diabetic_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Standardize missing values\n",
    "diabetic_df.replace(\"?\", pd.NA, inplace=True)\n",
    "\n",
    "# Ensure that 'admission_type_id' is stored as string in both datasets to avoid merge errors.\n",
    "diabetic_df['admission_type_id'] = diabetic_df['admission_type_id'].astype(str)\n",
    "ids_mapping_df['admission_type_id'] = ids_mapping_df['admission_type_id'].astype(str)\n",
    "\n",
    "# Prevents row duplication during merge by keeping only the first occurrence.\n",
    "ids_mapping_df = ids_mapping_df.drop_duplicates(subset='admission_type_id', keep='first')\n",
    "\n",
    "#Merge to bring in descriptive admission type labels alongside patient records.\n",
    "merged_df = diabetic_df.merge(ids_mapping_df, on='admission_type_id', how='left')\n",
    "\n",
    "#Convert categorical columns to numerical codes.\n",
    "for column in merged_df.columns:\n",
    "    if merged_df[column].dtype == 'object' or merged_df[column].dtype.name == 'category':\n",
    "        merged_df[column] = merged_df[column].astype('category').cat.codes\n",
    "\n",
    "# Replace -1 (code for NaN) with actual NaN for clarity.\n",
    "for column in merged_df.columns:\n",
    "    if (merged_df[column] == -1).any():\n",
    "        merged_df[column] = merged_df[column].replace(-1, pd.NA)\n",
    "\n",
    "#Remove duplicate rows\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "\n",
    "#Save the cleaned dataset\n",
    "cleaned_output_file_path = 'Dataset/MergedDataset/diabetic_data_cleaned_for_Imputation.csv'\n",
    "merged_df.to_csv(cleaned_output_file_path, index=False)\n",
    "\n",
    "logging.info(f\" Cleaned dataset saved to: {cleaned_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02aed7",
   "metadata": {},
   "source": [
    "# Imputation Phase #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf2cae",
   "metadata": {},
   "source": [
    "## KNN Imputation\n",
    "\n",
    "To ensure our GAN models receive complete datasets, we perform **K-Nearest Neighbors (KNN) imputation** on the cleaned UCI Diabetes dataset.  \n",
    "KNN imputation works by finding the `k` most similar rows (neighbors) for each missing value and imputing the value based on their average. \n",
    "\n",
    "\n",
    "### Key Steps:\n",
    "1. Load the cleaned dataset.\n",
    "2. Preserve certain identifier columns that should not be imputed.\n",
    "3. Apply KNN imputation to the remaining features.\n",
    "4. Round encoded categorical variables to restore valid category integers.\n",
    "5. Merge imputed data with preserved columns and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f4030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN imputed dataset saved to: ImputedData/diabetic_data_KNN_imputed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import logging\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#Loading the cleaned dataset\n",
    "dataSet_file_path = \"Dataset/MergedDataset/diabetic_data_cleaned_for_Imputation.csv\"\n",
    "df = pd.read_csv(dataSet_file_path)\n",
    "\n",
    "#Preserving identifier columns\n",
    "preserve_cols = [\"encounter_id\", \"patient_nbr\", \"examide\", \"citoglipton\"]\n",
    "preserved_df = df[preserve_cols]\n",
    "impute_df = df.drop(columns=preserve_cols)\n",
    "\n",
    "#Apply KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "knn_imputed = knn_imputer.fit_transform(impute_df)\n",
    "\n",
    "# Convert imputed NumPy array back to DataFrame\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed, columns=impute_df.columns)\n",
    "\n",
    "\n",
    "#Round categorical columns\n",
    "categorical_columns_to_round = [\n",
    "    'race', 'gender', 'max_glu_serum', 'A1Cresult', 'readmitted',\n",
    "    'change', 'diabetesMed', 'glipizide', 'glimepiride', 'chlorpropamide',\n",
    "    'repaglinide', 'metformin', 'pioglitazone', 'acarbose', 'miglitol',\n",
    "    'glyburide', 'insulin', 'glyburide-metformin', 'rosiglitazone', 'nateglinide',\n",
    "    'tolazamide', 'tolbutamide', 'acetohexamide', 'troglitazone',\n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone', 'glipizide-metformin',\n",
    "    'glimepiride-pioglitazone', 'examide', 'citoglipton'\n",
    "]\n",
    "\n",
    "for column in categorical_columns_to_round:\n",
    "    if column in knn_imputed_df.columns:\n",
    "        knn_imputed_df[column] = knn_imputed_df[column].round(0).astype(int)\n",
    "\n",
    "#Merge preserved and imputed data\n",
    "final_knn_imp_df = pd.concat([preserved_df.reset_index(drop=True), knn_imputed_df], axis=1)\n",
    "\n",
    "#save the imputed dataset\n",
    "imp_file_path = \"ImputedData/diabetic_data_KNN_imputed.csv\"\n",
    "final_knn_imp_df.to_csv(imp_file_path, index=False)\n",
    "\n",
    "logging.info(f\"KNN imputed dataset saved to: {imp_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b3b44",
   "metadata": {},
   "source": [
    "## Handling Missing Values: Mean Imputation\n",
    "\n",
    "In this step, we address missing values using **Mean Imputation**.  \n",
    "This approach replaces each missing value in a numeric column with the **average value** of that column, preserving the overall distribution while ensuring no missing entries remain.  \n",
    "\n",
    "**Process Overview:**\n",
    "1. Load the cleaned dataset.\n",
    "2. Preserve identifier columns that must remain unchanged.\n",
    "3. Apply mean imputation to the remaining features.\n",
    "4. Round categorical encoded features back to valid integer categories.\n",
    "5. Save the fully imputed dataset for downstream GAN training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310cdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean imputed dataset saved to: ImputedData/diabetic_data_MEAN_imputed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.impute import SimpleImputer\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Loading the cleaned dataset\n",
    "dataSet_file_path = \"Dataset/MergedDataset/diabetic_data_cleaned_for_Imputation.csv\"\n",
    "df = pd.read_csv(dataSet_file_path)\n",
    "\n",
    "# Preserving identifier columns\n",
    "preserve_cols = [\"encounter_id\", \"patient_nbr\", \"examide\", \"citoglipton\"]\n",
    "preserved_df = df[preserve_cols]\n",
    "impute_df = df.drop(columns=preserve_cols)\n",
    "\n",
    "# Performing mean imputation on remaining features\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "mean_imputed = mean_imputer.fit_transform(impute_df)\n",
    "mean_imputed_df = pd.DataFrame(mean_imputed, columns=impute_df.columns)\n",
    "\n",
    "# Rounding categorical columns back to integers\n",
    "categorical_columns_to_round = [\n",
    "    'race', 'gender', 'max_glu_serum', 'A1Cresult', 'readmitted',\n",
    "    'change', 'diabetesMed', 'glipizide', 'glimepiride', 'chlorpropamide',\n",
    "    'repaglinide', 'metformin', 'pioglitazone', 'acarbose', 'miglitol',\n",
    "    'glyburide', 'insulin', 'glyburide-metformin', 'rosiglitazone', 'nateglinide',\n",
    "    'tolazamide', 'tolbutamide', 'acetohexamide', 'troglitazone',\n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone', 'glipizide-metformin',\n",
    "    'glimepiride-pioglitazone', 'examide', 'citoglipton'\n",
    "]\n",
    "\n",
    "for column in categorical_columns_to_round:\n",
    "    if column in mean_imputed_df.columns:\n",
    "        mean_imputed_df[column] = mean_imputed_df[column].round(0).astype(int)\n",
    "\n",
    "# Combining the preserved identifier columns with the imputed dataset\n",
    "final_mean_imp_df = pd.concat([preserved_df.reset_index(drop=True), mean_imputed_df], axis=1)\n",
    "\n",
    "# Saving the final mean-imputed dataset\n",
    "imp_file_path = \"ImputedData/diabetic_data_MEAN_imputed.csv\"\n",
    "final_mean_imp_df.to_csv(imp_file_path, index=False)\n",
    "\n",
    "logging.info(f\"Mean imputed dataset saved to: {imp_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e571f3",
   "metadata": {},
   "source": [
    "## GAIN Imputation\n",
    "\n",
    "In this step, we apply **Generative Adversarial Imputation Networks (GAIN)**\n",
    "\n",
    "A GAN-based deep learning method designed to accurately fill in missing values by learning the underlying data distribution.  \n",
    "Unlike simpler imputation methods (mean, KNN), GAIN uses an adversarial training process between a **Generator** which imputes missing data and a **Discriminator** which tries to distinguish between real and imputed values.\n",
    "  \n",
    "This approach helps produce more realistic and statistically consistent imputations.\n",
    "\n",
    "**Process Overview:**\n",
    "1. Load the cleaned dataset.\n",
    "2. Normalize data while retaining missing value masks.\n",
    "3. Define GAIN’s generator and discriminator networks.\n",
    "4. Train GAIN using adversarial learning.\n",
    "5. Reverse normalization and round categorical values.\n",
    "6. Save the final imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42920db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging\n",
    "\n",
    "# Disable TensorFlow v2 behavior for compatibility\n",
    "tf.disable_v2_behavior()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "#Load and Prepare the Dataset\n",
    "\n",
    "dataset_path = \"Dataset/MergedDataset/diabetic_data_cleaned_for_Imputation.csv\"\n",
    "diabetes_df = pd.read_csv(dataset_path)\n",
    "logging.info(f\"Dataset loaded: {diabetes_df.shape} rows and columns\")\n",
    "\n",
    "# Normalize dataset and create missing value mask\n",
    "scaler = MinMaxScaler()\n",
    "dataset_array = diabetes_df.to_numpy()\n",
    "normalized_data = scaler.fit_transform(pd.DataFrame(dataset_array).fillna(0))\n",
    "missing_value_mask = 1 - np.isnan(dataset_array).astype(int)\n",
    "logging.info(\"Data normalized and missing value mask created.\")\n",
    "\n",
    "\n",
    "#Define GAIN Generator and Discriminator\n",
    "#Generator for imputing missing values.\n",
    "def generator(input_data, mask, feature_dim):\n",
    "    inputs = tf.concat([input_data, mask], axis=1)\n",
    "    hidden_layer1 = tf.keras.layers.Dense(units=feature_dim, activation=tf.nn.relu)(inputs)\n",
    "    hidden_layer2 = tf.keras.layers.Dense(units=feature_dim, activation=tf.nn.relu)(hidden_layer1)\n",
    "    generated_output = tf.keras.layers.Dense(units=feature_dim, activation=tf.nn.sigmoid)(hidden_layer2)\n",
    "    return generated_output\n",
    "\n",
    "#Discriminator to distinguish observed vs. imputed values.\n",
    "def discriminator(imputed_data, hint, feature_dim):\n",
    "    inputs = tf.concat([imputed_data, hint], axis=1)\n",
    "    hidden_layer1 = tf.keras.layers.Dense(units=feature_dim, activation=tf.nn.relu)(inputs)\n",
    "    hidden_layer2 = tf.keras.layers.Dense(units=feature_dim, activation=tf.nn.relu)(hidden_layer1)\n",
    "    logits = tf.keras.layers.Dense(units=feature_dim)(hidden_layer2)\n",
    "    prob_output = tf.nn.sigmoid(logits)\n",
    "    return prob_output\n",
    "\n",
    "\n",
    "#GAIN training Function\n",
    "def gain(imperfect_data, gain_params):\n",
    "    batch_size = gain_params['batch_size']\n",
    "    hint_rate = gain_params['hint_rate']\n",
    "    alpha = gain_params['alpha']\n",
    "    max_iterations = gain_params['iterations']\n",
    "\n",
    "    num_samples, num_features = imperfect_data.shape\n",
    "    missing_mask = 1 - np.isnan(imperfect_data).astype(int)\n",
    "\n",
    "    # Placeholders\n",
    "    X_placeholder = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "    M_placeholder = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "    H_placeholder = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "\n",
    "    # Model architecture\n",
    "    generated_samples = generator(X_placeholder, M_placeholder, num_features)\n",
    "    combined_data = X_placeholder * M_placeholder + generated_samples * (1 - M_placeholder)\n",
    "    discriminator_probs = discriminator(combined_data, H_placeholder, num_features)\n",
    "\n",
    "    # Loss Functions\n",
    "    discriminator_loss = -tf.reduce_mean(\n",
    "        M_placeholder * tf.math.log(discriminator_probs + 1e-8) +\n",
    "        (1 - M_placeholder) * tf.math.log(1. - discriminator_probs + 1e-8)\n",
    "    )\n",
    "    generator_loss_unsupervised = -tf.reduce_mean((1 - M_placeholder) * tf.math.log(discriminator_probs + 1e-8))\n",
    "    mse_loss = tf.reduce_mean((M_placeholder * X_placeholder - M_placeholder * generated_samples) ** 2) / tf.reduce_mean(M_placeholder)\n",
    "    generator_loss = generator_loss_unsupervised + alpha * mse_loss\n",
    "\n",
    "    # Optimizers\n",
    "    discriminator_optimizer = tf.train.AdamOptimizer().minimize(discriminator_loss)\n",
    "    generator_optimizer = tf.train.AdamOptimizer().minimize(generator_loss)\n",
    "\n",
    "    # TensorFlow session\n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    logging.info(\"Starting GAIN training...\")\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        batch_indices = np.random.choice(num_samples, batch_size, replace=False)\n",
    "        batch_data = imperfect_data[batch_indices, :]\n",
    "        batch_mask = missing_mask[batch_indices, :]\n",
    "\n",
    "        noise = np.random.uniform(0, 0.01, size=[batch_size, num_features])\n",
    "        batch_data = np.nan_to_num(batch_data, nan=0.0)\n",
    "        batch_data = batch_mask * batch_data + (1 - batch_mask) * noise\n",
    "\n",
    "        hint_matrix_temp = np.random.binomial(1, hint_rate, size=[batch_size, num_features])\n",
    "        hint_matrix = batch_mask * hint_matrix_temp\n",
    "\n",
    "        session.run(discriminator_optimizer, feed_dict={\n",
    "            X_placeholder: batch_data, M_placeholder: batch_mask, H_placeholder: hint_matrix\n",
    "        })\n",
    "        session.run(generator_optimizer, feed_dict={\n",
    "            X_placeholder: batch_data, M_placeholder: batch_mask, H_placeholder: hint_matrix\n",
    "        })\n",
    "\n",
    "        if iteration % 1000 == 0:\n",
    "            logging.info(f\"Iteration {iteration}/{max_iterations} completed.\")\n",
    "\n",
    "    logging.info(\"GAIN training completed.\")\n",
    "\n",
    "    # Final Imputation pass\n",
    "    noise_full = np.random.uniform(0, 0.01, size=[num_samples, num_features])\n",
    "    filled_data = np.nan_to_num(imperfect_data, nan=0.0)\n",
    "    filled_data = missing_mask * filled_data + (1 - missing_mask) * noise_full\n",
    "\n",
    "    imputed_result = session.run(generated_samples, feed_dict={\n",
    "        X_placeholder: filled_data, M_placeholder: missing_mask\n",
    "    })\n",
    "    completed_data = imperfect_data.copy()\n",
    "    completed_data[np.isnan(imperfect_data)] = imputed_result[np.isnan(imperfect_data)]\n",
    "\n",
    "    session.close()\n",
    "    return completed_data\n",
    "\n",
    "\n",
    "# Run GAIN imputation\n",
    "gain_hyperparameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.5,\n",
    "    'alpha': 10,\n",
    "    'iterations': 20000\n",
    "}\n",
    "logging.info(\"Running GAIN imputation...\")\n",
    "gain_imputed_data = gain(normalized_data.copy(), gain_hyperparameters)\n",
    "\n",
    "# Reverse normalization\n",
    "gain_imputed_df = pd.DataFrame(scaler.inverse_transform(gain_imputed_data), columns=diabetes_df.columns)\n",
    "\n",
    "# Round categorical columns back to integers\n",
    "categorical_columns = [\n",
    "    'race', 'gender', 'max_glu_serum', 'A1Cresult', 'readmitted',\n",
    "    'change', 'diabetesMed', 'glipizide', 'glimepiride', 'chlorpropamide',\n",
    "    'repaglinide', 'metformin', 'pioglitazone', 'acarbose', 'miglitol',\n",
    "    'glyburide', 'insulin', 'glyburide-metformin', 'rosiglitazone', 'nateglinide',\n",
    "    'tolazamide', 'tolbutamide', 'acetohexamide', 'troglitazone',\n",
    "    'metformin-rosiglitazone', 'metformin-pioglitazone', 'glipizide-metformin',\n",
    "    'glimepiride-pioglitazone', 'examide', 'citoglipton'\n",
    "]\n",
    "for col in categorical_columns:\n",
    "    if col in gain_imputed_df.columns:\n",
    "        gain_imputed_df[col] = gain_imputed_df[col].round().astype(int)\n",
    "\n",
    "# Save the final imputed dataset\n",
    "output_path = \"ImputedData/diabetic_data_GAIN_imputed.csv\"\n",
    "gain_imputed_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"GAIN-imputed dataset saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf76047",
   "metadata": {},
   "source": [
    "## Generating Synthetic Medical Data with CTGAN\n",
    "\n",
    "In this section, we will leverage the **CTGAN (Conditional Tabular GAN)** model from the SDV Synthetic Data Vault library to generate high-quality synthetic patient records.  \n",
    "CTGAN is particularly effective for handling **mixed-type tabular data** with both continuous and categorical features, making it suitable for our medical dataset.  \n",
    "\n",
    "We will:\n",
    "1. Load the imputed dataset (GAIN, KNN, or Mean).\n",
    "2. Define the metadata and specify discrete (categorical) columns.\n",
    "3. Train the CTGAN model on the dataset.\n",
    "4. Generate synthetic records in batches.\n",
    "5. Save the synthetic dataset for downstream evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Change to DEBUG for more detailed logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "def generate_synthetic_dataset(imputed_file_path, imputation_method):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ensure GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        logging.info(f\"CUDA enabled device: {torch.cuda.get_device_name(0)}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        logging.warning(\"CUDA is not available. Falling back to CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Load imputed dataset\n",
    "    logging.info(f\"Loading {imputation_method} imputed dataset\")\n",
    "    imputed_df = pd.read_csv(imputed_file_path)\n",
    "\n",
    "    # Preserve identifier columns\n",
    "    preserve_cols = [\"encounter_id\", \"patient_nbr\", \"examide\", \"citoglipton\"]\n",
    "    preserved_df = imputed_df.drop(columns=preserve_cols, errors='ignore')\n",
    "    preserved_identifiers_df = imputed_df[preserve_cols].reset_index(drop=True)\n",
    "\n",
    "    # Detect metadata\n",
    "    table_metadata = SingleTableMetadata()\n",
    "    table_metadata.detect_from_dataframe(preserved_df)\n",
    "\n",
    "    # Mark categorical columns\n",
    "    categorical_columns = [\n",
    "        'admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
    "        'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "        'num_medications', 'number_outpatient', 'number_emergency',\n",
    "        'number_inpatient', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
    "        'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "        'glimepiride', 'acetohexamide', 'glipizide', 'glyburide',\n",
    "        'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "        'miglitol', 'troglitazone', 'tolazamide', 'insulin',\n",
    "        'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone',\n",
    "        'metformin-rosiglitazone', 'metformin-pioglitazone',\n",
    "        'change', 'diabetesMed', 'readmitted', 'description'\n",
    "    ]\n",
    "    for column in categorical_columns:\n",
    "        if column in preserved_df.columns:\n",
    "            table_metadata.update_column(column_name=column, sdtype='categorical')\n",
    "\n",
    "    # Initialize CTGAN synthesizer\n",
    "    synthesizer = CTGANSynthesizer(\n",
    "        table_metadata,\n",
    "        epochs=300,\n",
    "        batch_size=500,\n",
    "        verbose=True,\n",
    "        log_frequency=10,\n",
    "    )\n",
    "\n",
    "    # Train CTGAN model\n",
    "    logging.info(f\"Training CTGAN on {imputation_method} imputed dataset\")\n",
    "    synthesizer.fit(preserved_df)\n",
    "\n",
    "    # Generate synthetic data in batches\n",
    "    logging.info(\"Generating synthetic dataset\")\n",
    "    rows_per_batch = 10000\n",
    "    total_rows = len(preserved_df)\n",
    "    synthetic_data_batches = []\n",
    "\n",
    "    for batch_start in tqdm(range(0, total_rows, rows_per_batch), desc=\"Generating Rows\"):\n",
    "        batch_count = min(rows_per_batch, total_rows - batch_start)\n",
    "        synthetic_data_batches.append(synthesizer.sample(num_rows=batch_count))\n",
    "\n",
    "    synthetic_data_df = pd.concat(synthetic_data_batches).reset_index(drop=True)\n",
    "\n",
    "    # Clean categorical columns\n",
    "    for column in categorical_columns:\n",
    "        if column in synthetic_data_df.columns:\n",
    "            synthetic_data_df[column] = (\n",
    "                synthetic_data_df[column]\n",
    "                .replace([np.inf, -np.inf], np.nan)\n",
    "                .fillna(-1)\n",
    "                .round()\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "    # Merge back identifiers\n",
    "    final_synthetic_df = pd.concat([preserved_identifiers_df, synthetic_data_df], axis=1)\n",
    "\n",
    "    # Save synthetic dataset\n",
    "    output_file_path = f\"SyntheticData/synthetic_diabetic_using_{imputation_method}_imp.csv\"\n",
    "    final_synthetic_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    elapsed_time = round(time.time() - start_time, 2)\n",
    "    logging.info(f\"Synthetic data saved to: {output_file_path}\")\n",
    "    logging.info(f\"Time taken: {elapsed_time} seconds\")\n",
    "\n",
    "\n",
    "# Generate synthetic datasets\n",
    "generate_synthetic_dataset(\"ImputedData/diabetic_data_KNN_imputed.csv\", \"KNN\")\n",
    "generate_synthetic_dataset(\"ImputedData/diabetic_data_MEAN_imputed.csv\", \"MEAN\")\n",
    "generate_synthetic_dataset(\"ImputedData/diabetic_data_GAIN_imputed.csv\", \"GAIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5fbc2",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline for Real vs Synthetic Datasets\n",
    "\n",
    "This script compares real imputed datasets with synthetic datasets generated\n",
    "using different imputation methods (GAIN, KNN, MEAN).\n",
    "\n",
    "It evaluates them on:\n",
    "    - Statistical similarity metrics (KS test, JS divergence, Wasserstein distance)\n",
    "    - Predictive performance metrics (TSTR, TRTS)\n",
    "    - Correlation structure comparison\n",
    "    - Visual sample comparisons\n",
    "\n",
    "Results are saved as CSVs and plots for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9657511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "REAL_DATA_DIR = \"ImputedData\"\n",
    "SYNTHETIC_DATA_DIR = \"SyntheticData\"\n",
    "RESULTS_DIR = \"results\"\n",
    "PLOTS_DIR = os.path.join(RESULTS_DIR, \"plots\")\n",
    "\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COLUMN = \"readmitted\"\n",
    "\n",
    "# File mapping\n",
    "file_pairs = {\n",
    "    \"GAIN\": (\"diabetic_data_GAIN_imputed.csv\", \"synthetic_diabetic_using_GAIN_imp.csv\"),\n",
    "    \"KNN\": (\"diabetic_data_KNN_imputed.csv\", \"synthetic_diabetic_using_KNN_imp.csv\"),\n",
    "    \"MEAN\": (\"diabetic_data_MEAN_imputed.csv\", \"synthetic_diabetic_using_MEAN_imp.csv\")\n",
    "}\n",
    "\n",
    "# Store results\n",
    "ks_results = []\n",
    "tstr_results = []\n",
    "\n",
    "#Ks Test Fucntion\n",
    "def ks_test(real, synthetic):\n",
    "    numeric_cols = real.select_dtypes(include=['number']).columns\n",
    "    stats = {}\n",
    "    for col in numeric_cols:\n",
    "        if real[col].isnull().all() or synthetic[col].isnull().all():\n",
    "            continue\n",
    "        ks_stat, _ = ks_2samp(real[col], synthetic[col])\n",
    "        stats[col] = ks_stat\n",
    "    valid_ks = list(stats.values())\n",
    "    avg_ks = sum(valid_ks) / len(valid_ks) if valid_ks else None\n",
    "    return avg_ks\n",
    "\n",
    "#TSTR Test Function\n",
    "def tstr(real_df, synth_df, target):\n",
    "    X_train = synth_df.drop(columns=[target])\n",
    "    y_train = synth_df[target]\n",
    "    X_test = real_df.drop(columns=[target])\n",
    "    y_test = real_df[target]\n",
    "\n",
    "    X_train = X_train[X_test.columns]  \n",
    "\n",
    "    model = RandomForestClassifier(random_state=SEED)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy_TSTR\": accuracy_score(y_test, y_pred),\n",
    "        \"F1_TSTR\": f1_score(y_test, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "#Main Loop\n",
    "for method, (real_file, synth_file) in file_pairs.items():\n",
    "    logging.info(f\"Processing {method}...\")\n",
    "\n",
    "    real_data = pd.read_csv(os.path.join(REAL_DATA_DIR, real_file))\n",
    "    synthetic_data = pd.read_csv(os.path.join(SYNTHETIC_DATA_DIR, synth_file))\n",
    "\n",
    "    # KS Test\n",
    "    avg_ks = ks_test(real_data, synthetic_data)\n",
    "    ks_results.append({\"Imputation\": method, \"Avg_KS\": avg_ks})\n",
    "\n",
    "    # TSTR\n",
    "    tstr_scores = tstr(real_data, synthetic_data, TARGET_COLUMN)\n",
    "    tstr_scores[\"Imputation\"] = method\n",
    "    tstr_results.append(tstr_scores)\n",
    "\n",
    "# Saving Results\n",
    "ks_df = pd.DataFrame(ks_results)\n",
    "tstr_df = pd.DataFrame(tstr_results)\n",
    "ks_df.to_csv(os.path.join(RESULTS_DIR, \"ks_scores.csv\"), index=False)\n",
    "tstr_df.to_csv(os.path.join(RESULTS_DIR, \"tstr_scores.csv\"), index=False)\n",
    "\n",
    "logging.info(\"KS scores and TSTR scores saved to CSV.\")\n",
    "\n",
    "# KS Score Visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=ks_df, x=\"Imputation\", y=\"Avg_KS\", palette=\"Blues_d\")\n",
    "plt.ylabel(\"Average KS Score\")\n",
    "plt.title(\"KS Score by Imputation\")\n",
    "plt.ylim(0, max(ks_df[\"Avg_KS\"]) + 0.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, \"ks_score_bar.png\"))\n",
    "plt.close()\n",
    "logging.info(\"KS score bar plot saved.\")\n",
    "\n",
    "# TSTR Scores Visualization\n",
    "tstr_melt = tstr_df.melt(id_vars=\"Imputation\", value_vars=[\"Accuracy_TSTR\", \"F1_TSTR\"])\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=tstr_melt, x=\"Imputation\", y=\"value\", hue=\"variable\", palette=\"Set2\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"TSTR Evaluation Metrics by Imputation\")\n",
    "plt.ylim(0, max(tstr_melt[\"value\"]) + 0.05)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, \"tstr_metrics_bar.png\"))\n",
    "plt.close()\n",
    "logging.info(\"TSTR metrics bar plot saved.\")\n",
    "\n",
    "# Combined Visualization\n",
    "combined_df = pd.merge(tstr_df, ks_df, on=\"Imputation\")\n",
    "combined_melt = combined_df.melt(id_vars=\"Imputation\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=combined_melt, x=\"Imputation\", y=\"Score\", hue=\"Metric\", palette=\"Set2\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"TSTR Metrics & KS Score by Imputation\", fontsize=14)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, \"tstr_ks_bar.png\"), dpi=300)\n",
    "plt.close()\n",
    "logging.info(\"Combined TSTR and KS metrics plot saved.\")\n",
    "\n",
    "logging.info(\"Evaluation complete TSTR and KS results saved combined plot generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
